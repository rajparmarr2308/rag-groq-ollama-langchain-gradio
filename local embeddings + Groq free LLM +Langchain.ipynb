{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1701b1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_community import embeddings\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "loader = PyPDFDirectoryLoader(\"data\")\n",
    "the_text = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(the_text)\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    collection_name=\"ollama_embeds\",\n",
    "    embedding=OllamaEmbeddings(model='nomic-embed-text'),\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "831db79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from env import groq_api_key\n",
    "llm = ChatGroq(\n",
    "            groq_api_key=groq_api_key,\n",
    "            model_name='mixtral-8x7b-32768'\n",
    "    )\n",
    "\n",
    "\n",
    "#https://github.com/ollama/ollama\n",
    "\n",
    "# from langchain_community.llms import Ollama\n",
    "\n",
    "# llm = Ollama(model=\"llama2\")\n",
    "\n",
    "# llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeb45281",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "rag_prompt = ChatPromptTemplate.from_template(rag_template)\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "789cde09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document is about the Indian Space Research Organization's (ISRO)\n",
      "Chandrayaan 3 mission, which is a follow-on mission to Chandrayaan 2. The\n",
      "primary objective of Chandrayaan 3 is to land a lander and rover in the\n",
      "highlands near the south pole of the Moon and demonstrate end-to-end landing and\n",
      "roving capabilities. The mission comprises a lander/rover and a propulsion\n",
      "module. The lander/rover is similar to the Vikram rover on Chandrayaan 2, with\n",
      "improvements to ensure a safe landing. The propulsion module carries the lander\n",
      "and rover configuration and will remain in orbit around the Moon while acting as\n",
      "a communications relay satellite. Chandrayaan 3 will make several scientific\n",
      "measurements on the lunar surface and from orbit, and it consists of an\n",
      "indigenous propulsion module (PM), a lander module (LM), and a rover. The PM\n",
      "carries the lander and rover from injection orbit to 100 km lunar orbit and\n",
      "separates the LM from the PM. The LM and the Rover have scientific payloads to\n",
      "carry out experiments on the lunar surface. The PM also has one scientific\n",
      "payload as a value addition which will operate post-separation of the Lander\n",
      "Module.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import textwrap\n",
    "import gradio as gr\n",
    "\n",
    "# Test the architecture with a simple hard coded question\n",
    "response = rag_chain.invoke(\"What is this document about\")\n",
    "print(textwrap.fill(response, width=80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8355a5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://4809b1dba260f61560.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://4809b1dba260f61560.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make the questions dynamic using a chat interface. Let's use gradio for this.\n",
    "def process_question(user_question):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Directly using the user's question as input for rag_chain.invoke\n",
    "    response = rag_chain.invoke(user_question)\n",
    "\n",
    "    # Measure the response time\n",
    "    end_time = time.time()\n",
    "    response_time = f\"Response time: {end_time - start_time:.2f} seconds.\"\n",
    "\n",
    "    # Combine the response and the response time into a single string\n",
    "    full_response = f\"{response}\\n\\n{response_time}\"\n",
    "\n",
    "    return full_response\n",
    "\n",
    "# Setup the Gradio interface\n",
    "iface = gr.Interface(fn=process_question,\n",
    "                     inputs=gr.Textbox(lines=2, placeholder=\"Type your question here...\"),\n",
    "                     outputs=gr.Textbox(),\n",
    "                     title=\"Personal Knowledge Chat App\",\n",
    "                     description=\"Ask any question about your document, and get an answer along with the response time.\")\n",
    "\n",
    "# Launch the interface\n",
    "iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2b41bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
